# -*- coding: utf-8 -*-
"""lr.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z6J64aiS3_m0YwSOQP0IaNajwaTTdlWH
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Load datasets
X = pd.read_csv('logisticX.csv', header=None).values
y = pd.read_csv('logisticY.csv', header=None).values

# Normalize the data
X_mean = np.mean(X, axis=0)
X_std = np.std(X, axis=0)
X = (X - X_mean) / X_std

# Add intercept term
X = np.hstack((np.ones((X.shape[0], 1)), X))

# Sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Cost function
def compute_cost(X, y, theta):
    m = len(y)
    h = sigmoid(X @ theta)
    cost = (-1/m) * (y.T @ np.log(h) + (1 - y).T @ np.log(1 - h))
    return cost[0, 0]

# Gradient descent function
def gradient_descent(X, y, theta, learning_rate, iterations):
    m = len(y)
    cost_history = []

    for _ in range(iterations):
        h = sigmoid(X @ theta)
        gradient = (1/m) * (X.T @ (h - y))
        theta -= learning_rate * gradient
        cost_history.append(compute_cost(X, y, theta))

    return theta, cost_history

# Initialize parameters
theta = np.zeros((X.shape[1], 1))
learning_rate_1 = 0.1
iterations = 100

# Train the model with learning rate = 0.1
theta_1, cost_history_1 = gradient_descent(X, y, theta, learning_rate_1, iterations)

# Train the model with learning rate = 5
learning_rate_2 = 5
theta_2, cost_history_2 = gradient_descent(X, y, theta, learning_rate_2, iterations)

# Plot cost function vs iterations
plt.plot(range(iterations), cost_history_1, label='Learning Rate = 0.1')
plt.plot(range(iterations), cost_history_2, label='Learning Rate = 5')
plt.xlabel('Iterations')
plt.ylabel('Cost Function')
plt.title('Cost Function vs Iterations')
plt.legend()
plt.show()

# Decision boundary plot
plt.figure()
for i in range(len(y)):
    if y[i] == 0:
        plt.plot(X[i, 1], X[i, 2], 'ro')  # Class 0
    else:
        plt.plot(X[i, 1], X[i, 2], 'bo')  # Class 1

# Plot the decision boundary
x_values = np.array([min(X[:, 1]), max(X[:, 1])])
y_values = -(theta_1[0] + theta_1[1] * x_values) / theta_1[2]
plt.plot(x_values, y_values, 'g-', label='Decision Boundary')
plt.xlabel('Normalized Feature 1')
plt.ylabel('Normalized Feature 2')
plt.title('Decision Boundary')
plt.legend()
plt.show()

# Confusion Matrix and Metrics
def predict(X, theta):
    return (sigmoid(X @ theta) >= 0.5).astype(int)

y_pred = predict(X, theta_1)

TP = np.sum((y_pred == 1) & (y == 1))
TN = np.sum((y_pred == 0) & (y == 0))
FP = np.sum((y_pred == 1) & (y == 0))
FN = np.sum((y_pred == 0) & (y == 1))

accuracy = (TP + TN) / len(y)
precision = TP / (TP + FP) if TP + FP > 0 else 0
recall = TP / (TP + FN) if TP + FN > 0 else 0
f1_score = (2 * precision * recall) / (precision + recall) if precision + recall > 0 else 0

print("Confusion Matrix:")
print(f"TP: {TP}, TN: {TN}, FP: {FP}, FN: {FN}")
print(f"Accuracy:  {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1_score:.4f}")
